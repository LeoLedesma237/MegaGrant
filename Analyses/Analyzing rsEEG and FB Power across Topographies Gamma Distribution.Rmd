---
title: "Analyzing resting-state EEG and Spelling Performance in Young Adults (Gamma)"
author: "Leandro Ledesma"
date: "2024-12-24"
output: html_document
---

### Universal block code settings

```{r setup}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(comment = NULL)
knitr::opts_chunk$set(warning = FALSE)

```

### Load in the data manipulation packages first

```{r loading in the packages, warning = FALSE}
library(tidyverse)
library(ggplot2)
library(readxl)
library(kableExtra)
library(broom) # Converts regression outputs into dataframes using the tidy() function
library(psych)
library(MASS, exclude = "select") # This package is loaded with QuantPsyc, must exclude "select" or you wont be able to use it. 
library(QuantPsyc) # Can use the lm.beta function to calculate the standardized betas
library(car) # To calculate VIF
library(performance) # ICC 
library(lme4) #glmer
library(interactions) # interact_plot
```

### Load in our data

```{r load in predictor variable, warning= FALSE}
# Set the working directory
Mega <- '\\\\files.times.uh.edu/labs/MIR_Lab/MEGAGRANT/STUDY 1/FINAL_DS'
setwd(Mega)

# load data
demo <- read_excel("Demo/MegaGrant_TBL_Database_Newest_MCh with duration.xlsx")
ARFA <- read.csv("ARFA/ARFA.Spelling.Errors.Scored.csv")
CFIT <- read.csv("CFIT/CFIT.scores.csv")
fftx <- read.csv("EEG/rsEEG/topographyFBAvgPowFFT.csv")
welchx <-  read.csv("EEG/rsEEG/topographyFBAvgPowWelch.csv")


# Data cleaning
CFIT <- select(CFIT, ID, IQRS = Raw.Scores)
ARFA <- select(ARFA, ID, Total_SpellingError)
demo <- demo %>%
  select(ID, 
         Sex, 
         Age, 
         Group) %>%
  mutate(ID = as.numeric(ID),
         Age = as.numeric(Age)) 

# EEG data cleaning
fftx$ID <- as.numeric(gsub("\\D", "",  sapply(str_split(fftx$filename,"_"), function(x) x[1])))
welchx$ID <- as.numeric(gsub("\\D", "",  sapply(str_split(welchx$filename,"_"), function(x) x[1])))
fftx <- select(fftx, -filename)
welchx <- select(welchx, -filename)
names(fftx) <- c(paste0(names(fftx)[1:(length(fftx)-1)],"_fftx"),"ID")
names(welchx) <- c(paste0(names(welchx)[1:(length(fftx)-1)],"_welchx"),"ID")


### Combine the dataset into one
data <- demo %>%
  full_join(CFIT, by = "ID") %>%
  full_join(ARFA, by = "ID") %>%
  full_join(fftx, by = "ID") %>%
  full_join(welchx, by = "ID")
```


### Index subjects by those that met the requirements

- No self-report injury
- No Epilepsy
- Removing missing data
- No one below 'IQ' of 70 (more than 2 SD below the mean)
- Extremely poor spellers (more than 3 SD below the mean)
- Issues with EEG recordings
- Did not survive EEG cleaning


```{r keep non rejects subjects}
# Set the working directory
setwd(Mega)

# Load in the final sample data
Final_Sample_Size <- read_excel("EEG/rsEEG/Final.Sample.Size.xlsx")

# Index the data by the IDs in the final sample size
data2 <- data %>%
  filter(ID %in% Final_Sample_Size$ID)

```


### Missing data

- All data should be present based on the index from the previous section


```{r reporting missing data part 1,  results= 'asis', echo = FALSE}
cat("This is an automated response: The current dataset has the dimensions",paste(dim(data2), collapse = " x "),". Below is a frequency table showing the number of missing data for each variable.")

# Create a dataframe of missing data
Missing.data.df <- cbind(colSums(is.na(data2))) %>%
  data.frame(Missing.Num = .)

# Add More variables of interest
Missing.data.df$Remaining.Num = nrow(data) - Missing.data.df$Missing.Num
Missing.data.df$Remaning.Per = round(Missing.data.df$Remaining.Num/nrow(data),3)*100

# Add a variable indicating which type of data is missing
Missing.data.df$data <- row.names(Missing.data.df)

# Remove row names to reduce redundancy
row.names(Missing.data.df) <- NULL

# Print the table
Missing.data.df %>%
  select(data, Missing.Num, everything()) %>%
  arrange(Missing.Num) %>% 
  kbl() %>%  
  kable_paper(full_width = F)
```


```{r reporting missing data part 2,  results= 'asis', echo = FALSE}
cat("This is an automated response: Our final sample size is n=",length(unique(data2$ID)))

```

## Research Question
- We are interested in investigating spelling performance as a predictor of EEG activity. We hypothesize that spelling ability will result in differences in brain activity while controlling for covariates like Age, Group (Bio vs Ins), and topography. 


## Descriptive statistics of the data

We can use the describe() function from the psych package to give us descriptive statistics from the variables that will be incorporated into the model. We will create 3 tables. The first will contain information about the continuous predictors. The next will have the frequency of the categorical variables. Lastly we will have a table for the resting-state EEG frequency bands. 

We will be creating models for 4 differenet frequency bands (delta, theta, alpha, beta), thus, four datasets will be created where only information related to a specific frequency band are present.

### Descriptives of our data

```{r descriptive statistics of continuous predictors}
# Puts it in scientific notation
options(scipen = 0)

# Select continuous variables
data2_cont <- data2 %>% select(ID, Age, IQRS, Total_SpellingError, frontal_avgdelta_fftx:occipital_avgbeta_welchx)

data.frame(describe(data2_cont)) %>%
  round(3) %>%
  kbl() %>%
  kable_minimal() 

# Categorical information
table(data2$Sex)
table(data2$Group)
```


### FFT vs Pwelch

- Let's compare the variance of power between FFT and Welch Method approach
- Must fix issue with parietal data in fftx- right now the comparison is not meaningful
- For right now we will conduct the rest of the analysis for Welch's Method

```{r comparing variance of power between FFT and Welch Method}
# Saving each type of EEG processing in their own datasets
EEGfftx <- select(data2, ID, frontal_avgdelta_fftx:occipital_avgbeta_fftx)
EEGWelchx <- select(data2, ID, frontal_avgdelta_welchx:occipital_avgbeta_welchx)

# Converting each to long
EEGfftxL <- EEGfftx %>%
  pivot_longer(-ID, names_to = "Var", values_to = "Power")
EEGWelchx <- EEGWelchx %>%
  pivot_longer(-ID, names_to = "Var", values_to = "Power")

# Introduce more variables
EEGfftxL <- EEGfftxL %>%
  mutate(Topography = sapply(str_split(Var,"_"), function(x) x[1]),
         FreqBand = sapply(str_split(Var,"_"), function(x) x[2]),
         Method = sapply(str_split(Var,"_"), function(x) x[3])) %>%
  select(-Var)

EEGWelchx <- EEGWelchx %>%
  mutate(Topography = sapply(str_split(Var,"_"), function(x) x[1]),
         FreqBand = sapply(str_split(Var,"_"), function(x) x[2]),
         Method = sapply(str_split(Var,"_"), function(x) x[3])) %>%
  select(-Var)

# Join them back together
EEGfftxWelch <- EEGfftxL %>%
  full_join(EEGWelchx, by = c("ID", "Topography", "FreqBand", "Method", "Power"))

# Group the variables to create an accurate z-score
EEGfftxWelch2 <- EEGfftxWelch %>%
  group_by(Topography, FreqBand, Method) %>%
  summarise(zscore = scale(Power))

# Create a comprehensive box plot of these data
EEGfftxWelch2 %>%
  ggplot(aes(x = Topography, y = zscore, fill = FreqBand)) +
  geom_boxplot() +
  facet_grid(~Method)

EEGfftxWelch3 <- EEGfftxWelch %>%
  filter(Power < 10000) %>%
  group_by(Topography, FreqBand, Method) %>%
  summarise(zscore = scale(Power))

# Create another boxplot removing that one outlier
EEGfftxWelch3 %>%
  ggplot(aes(x = Topography, y = zscore, fill = FreqBand)) +
  geom_boxplot() +
  facet_grid(~Method)
```



### Visualizing all continuous variables as histograms (Z-scores)

- This drives home the message that the outcome variable is highly skewed to the right in all possible combinations of FB and Topography.

```{r visualize all variables as histograms, out.width= "33%"}
# Removing fftx variables
data3_cont <- data2_cont %>%
  select(Age:Total_SpellingError, frontal_avgdelta_welchx:occipital_avgbeta_welchx)

# Plot the histograms of all continuous variables
histograms.list <- list()

for(ii in 1:length(data3_cont)) {

  # Save the current var into an object
  current_var <- unlist(data3_cont[,ii])
  
  # Convert it into a z-score
  current_var_z <- scale(current_var)

  # Plot it
  current_plot <- current_var_z %>%
    data.frame(x=.) %>%
      ggplot(aes (x = x)) +
      geom_histogram(fill = "white",
                     color = "black",
                     bins = 20) +
      labs(title = paste0(names(data3_cont[,ii]),"_z")) +
      theme(plot.title = element_text(size = 14, hjust = 0.5)) +
    theme_classic()

  # Print out the plot
  plot(current_plot)
}

```


### Testing for Multicollinearity

- We need to drop predictors that are highly correlated with each other
- Raw scores and total spelling error is kinda high but I think okay

```{r run a correlation matrix}
cor(select(data3_cont, Age, IQRS, Total_SpellingError))

```


### Check for independence between categorical variables

P-value is significant. It seems there are more females who were raised in biological families than males. This may not be as big of a deal as initially thought.  

```{r run a chi square test}
# Create a contingency table
contingency.table <- table(select(data2, Sex, Group ))

# Show the table with margins
addmargins(contingency.table)

# Obtain the proportions for each cell
round(prop.table(contingency.table),3)

# Run chi-square test
chisq.test(contingency.table)
```
### Preparing Data for Modeling

We see here that the z-score distribution is larger, but that is highly likely to having power from different brain regions all in one vector AND frequency bands. We can confirm this with boxplots.

```{r preparing data for modeling}
# Converting the data into long format to isolate topography and FB
data3 <- data2 %>%
  select(ID:Total_SpellingError, frontal_avgdelta_welchx:occipital_avgbeta_welchx) %>%
  pivot_longer(cols= c(frontal_avgdelta_welchx:occipital_avgbeta_welchx),
               names_to = "Var", values_to = "Power")

# Separate 'name' into  two variables
data3$Topography <- sapply(str_split(data3$Var, "_"), function(x) x[1])
data3$FreqBand <- sapply(str_split(data3$Var, "_"), function(x) x[2])
data3 <- select(data3, - Var)

# Create a box plot to see brain activity across regions
data3 %>%
  ggplot(aes(x = FreqBand ,y= Power, color = Topography)) +
  geom_boxplot() +
  theme_classic() +
  coord_flip()

```
### Running parallel processing

Unsure if this actually is improving the computational speed of the model. I also don't feel like testing it. 

```{r running parallel processing}
library(future)
plan(multicore, workers = 12)  # Set number of workers (check your specs); use 'multisession' for windows or 'multicore' for Mac/Linux

```



### Another Multicollinearity Test (VIF on the model)


A better approach to measure **multicollinearity** is by using the variance inflation factor. We can obtain this value by first running a model. We will run a model here with our main predictors of interest- not interactions will be used.


```{r another multicollinearity test}
# Center all continuous predictor- not needed now but will be important during interactions
data3$Age_z <- c(scale(data3$Age))
data3$Total_SpellingError_z <- c(scale(data3$Total_SpellingError))
data3$IQRS_z <- c(scale(data3$IQRS))


# Run the model with all potential main effects to be included
tic()
muticollinearity_mod <-  glmer(Power ~ Age_z + Sex + Group + IQRS_z + Total_SpellingError_z + Topography + FreqBand + (1|ID), data = data3, 
                            family = Gamma(link = "log"))
toc()

# Obtain a vif score
vif(muticollinearity_mod)

```
Looking at the **variance inflation factor (VIF)** of each predictor- there seems to be no multicollinearity so we are good to continue. 


### Which Predictors to keep

```{r identifying which predictors from above to keep}

# Investigate the main effects
Anova(muticollinearity_mod, type = "III")

```
Significant: Age_z, Topography, FreqBand

Not-significant: Sex, Group, IQRS_c, Total_SpellingError_z

Since Total_SpellingError_z is vital to the research question, it will remain as a predictor and then used for interactions with
Age_z, Topography, and Frequency Band. 

### Having Frequency Band as a Factor or an Outcome?

In a perfect world, it would make sense to have Frequency Band as a Factor in the model for a few reasons. At face value, it is another predictor added to the model which could explain the variance of the outcome and we can use it to test higher order interactions. Thus, it has the opportunity to provides a wealth of information. There are however serious problems with having frequency bands as predictor. The model already has three predictors and in order to test our research question fully, then we would have to create a 4 way-interaction. This is problematic because it overcomplicated the model by a large margin and we do not have the sample size to be able to investigate it thoroughly. We have 449 observations, thus the rule of thumb is we should have one predictor for every 10-20 observations in our data. A 4-way interaction produces 66 free parameters, which is way more than we can handle. Also, when we plotted the data a few sections back, we saw that the variance of power was similar within frequency bands but highly differed across frequency bands- this could cause potential problems in our model and violated the assumption of heterogeneity. Lastly, if we create four models, each predicting their respective frequency band power outcome, then we could use three-way interactions and just compare trends of them in post without needing to use statistics. 

```{r splitting the data by frequency band}
# Remove uncentered predictors (QC)
data4 <- select(data3, -c(Age,Total_SpellingError ))

# Create a dataset for each FrequencyBand
delta <- filter(data4, FreqBand == "avgdelta")
theta <- filter(data4, FreqBand == "avgtheta") 
alpha <- filter(data4, FreqBand == "avgalpha")
beta <- filter(data4, FreqBand == "avgbeta")

# Put these datasets into a list
FBdat <- list(delta, theta, alpha, beta)
```


### Running our models

As mentioned, we are interested in how spelling performance may potentially influence electrical brain activity. To do this, we will be creating a **generalized linear mixed effects model (GLMM)** with a gamma distribution and **log** link function. This is because the distribution of the outcome power (which is a value representing averaged amplitudes within a frequency range/band squared) is positive and skewed to the right, which will produced skewed residuals. A Gamma distribution can better fit our data. Additionally, the introduction of topographical regions as a factor requires subjects to have their own random intercept. This is because we may expect brain regions to be more similar to each other within a person than across- thus, this needs to be taken into account by introducing subjects as a random effect. 

The log link part is important and different from a logit link. The similarity is that exp() is used to represent these values into a number that is easier to understand. For the 'log' link, exp() converts it into a **raw score**- so in a way, it seems that exp() estimates is like turned them back into regular estimates that can be interpreted linearly (as one unit increase we expect this increase for the outcome) instead of as a probability. This is information obtain from ChatGPT.


We will use model comparison to see which model would be the best to interpret- however, if the model of best fit is too difficulty/complex, then we may chose the second best model.

Model 0: Empty model with power as the outcome
Model 1: A model with the covariates: They will also go through model comparison to pick the best coviarate model
Model 2: A model with the chosen covariates and Topography
Model 3: A model with the chosen covariates, Topography, and the predictor of interest (Spelling Error)
Model 4: Complex model with interactions (Spelling Error x Age)
Model 5: Complex model with interactions (Spelling Error x Topography)
Model 6: A very complex model with interactions (Spelling Error x Topography X Age)


### Comparing delta models

```{r running the analyses, out.width  = "49%"}
# Create a list to hold the model outputs
mods0 <- list()
mods1 <- list()
mods2 <- list()
mods3 <- list()
mods4 <- list()
mods5 <- list()
mods6 <- list()

for (ii in 1:length(FBdat)) {
tic()
# Creating the models
mods0[[ii]] <- glmer(Power ~ 1 + (1|ID), data = FBdat[[ii]], 
            family = Gamma(link = "log")) 


# Create the six models
mods1[[ii]] <- glmer(Power ~ Age_z + (1|ID), 
                     data = FBdat[[ii]], 
                     family = Gamma(link = "log"))

mods2[[ii]] <- glmer(Power ~ Age_z + Topography + (1|ID), 
                     data = FBdat[[ii]], 
                     family = Gamma(link = "log"))

mods3[[ii]] <- glmer(Power ~ Age_z + Topography  + Total_SpellingError_z + (1|ID), 
                     data = FBdat[[ii]], 
                     family = Gamma(link = "log"))

mods4[[ii]] <- glmer(Power ~ Age_z + Topography + Age_z*Total_SpellingError_z + (1|ID), 
                     data = FBdat[[ii]], 
                     family = Gamma(link = "log"))

mods5[[ii]] <- glmer(Power ~ Age_z + Topography + Age_z*Total_SpellingError_z + Topography*Total_SpellingError_z + (1|ID),
                     data = FBdat[[ii]], 
                     family = Gamma(link = "log"),
                     control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1500000)))

mods6[[ii]] <- glmer(Power ~ Age_z * Total_SpellingError_z * Topography + (1|ID), 
                     data = FBdat[[ii]], 
                     family = Gamma(link = "log"),
                     control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1500000)))
toc()

}

# model comparison results
modcomp <- list()

for(ii in 1:length(FBdat)) {
modcomp[[ii]] = do.call(anova, c(mods0[[ii]], mods1[[ii]], mods2[[ii]], mods3[[ii]], mods4[[ii]], mods5[[ii]], mods6[[ii]], list(test = "Chisq")))

}

# Give the modeloutput names
names(modcomp) <- c("delta", "theta", "alpha", "beta")

# Print out the model comparisons
modcomp
```




## Model comparison interpretation

As mentioned, eight models were compared. These models were nested, meaning each more complex model builds on a simpler one by adding predictors. This allows us to assess whether each predictor meaningfully improves model fit. If adding a predictor does not significantly improve the model (suggesting no main effect or a weak effect size), but an interaction involving that predictor is significant, then that would suggest that variance can be better explain by introducing that predictor after all to create the interaction!

Using the `anova()` function, we are able to get several model fit statistics.

- **Number of free parameters (npar)**: This is a measurement of model complexity- the larger the number the more complex the model is. A rule of thumb is to not have between 10-20 observations for each parameter in the model. Since I have a sample size of 449 subjects, having a npar value around 20 is very reasonable. 

- **Akaike Information Criterion (AIC)**: A value where the lower it is the better the model fit. Penalizes models for being too complex. 

- **Bayesian Information Criterion (BIC)**: This is similar to AIC but penalizes complex models to a stronger degree. A smaller number also indicates a better fitting model.  

- **logLik**: A less negative value indicates better fit

- **deviance**: How much the model deviates from a perfect fit. A lower deviance is better.

- **Chisq**: a chi-square test. A higher chi-square means the added predictors improved the model. 

- **Df**: The number of additional parameters in the more complex model compared to the simpler one

- **Pr**: The p-value of the chi-square test. 


**Model 6** is the best:

Which Model is Best?
We compare models based on AIC, BIC, and significance tests to find a good balance between fit and complexity. We see that **mod8** fits the best because it has lowest AIC (22879), lowest BIC (23023), lowest deviance (22837) and a Chi-square test indicates it is a significantly better model than the previous model, but really tahn all those that came before it. 


### Model 8 Main Effects

```{r lets explore the main effects of our best fitting model}
# Report the main effects using type III Sums os Squares
Anova(mods6[[1]], type ="III")
Anova(mods6[[2]], type ="III")
Anova(mods6[[3]], type ="III")
Anova(mods6[[4]], type ="III")

```

We see here that everything is significant. One may think to adjust the p-values here- to control for multiple comparison. I will not be doing that for a couple of reasons.

1. Multiple comparison for me makes the most sesne when controlling for the comparisons between levels of a factor- this is not
the case here in this example.

2. Using model comparison we know already that this model is good- so introducing a p-value adjustment would turn this model back
into an inferior one.

3. I am like 80% sure Dr. Francis mentioned we did not have to do p-value adjusments here. 


### Investigating the interactions

```{r investigating the interactions}
library(interactions)
interact_plot(mods6[[1]], pred = "Age_z", modx = "Total_SpellingError_z", mod2 = "Topography")
interact_plot(mods6[[2]], pred = "Age_z", modx = "Total_SpellingError_z", mod2 = "Topography")
interact_plot(mods6[[3]], pred = "Age_z", modx = "Total_SpellingError_z", mod2 = "Topography")
interact_plot(mods6[[4]], pred = "Age_z", modx = "Total_SpellingError_z", mod2 = "Topography")
```




### Comparing Gamma vs Gaussian


```{r plotting residuals of gamma and gaussian}
# Creating the Guassian counterpart to compare distribution of the errors
mod8_guassian <- glm(Power ~  Topography + FreqBand * Age_z * Total_SpellingError_z + (1|ID), data = data4, 
            family = gaussian(link = "identity"))

# Plotting the residuals
qqnorm(resid(mod8))
qqline(resid(mod8))
hist(resid(mod8))
qqnorm(resid(mod8_guassian))
qqline(resid(mod8_guassian))
hist(resid(mod8_guassian))
```